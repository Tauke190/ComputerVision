{"cells":[{"cell_type":"markdown","metadata":{"id":"t_KVysdl_eM_"},"source":["### 0.Import some packages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12135,"status":"ok","timestamp":1743405890209,"user":{"displayName":"Niraj Pudasaini","userId":"08773241725886419596"},"user_tz":-240},"id":"z4n__ZZi2nZo","outputId":"39b4560e-a3ce-4c0e-943a-a07e41738209"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.25.0 in /usr/local/lib/python3.11/dist-packages (1.25.0)\n"]}],"source":["from keras.datasets import mnist\n","from PIL import Image\n","!pip install numpy==1.25.0\n","import numpy as np\n","np.random.seed(10) #for reproduceability\n"]},{"cell_type":"markdown","metadata":{"id":"OlZEEERH_lZ7"},"source":["### 1. Define  Image correlation and convolution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQzPJJiP2qh0"},"outputs":[],"source":["#this function is used for forward pass\n","def image_correlation(input, kernel, mode='valid'):\n","    # 'valid' ensures no padding (output size = input_size - kernel_size + 1).\n","    # 28 x 28 --> 26 x 26\n","    output_shape = [input.shape[0] - kernel.shape[0] + 1, input.shape[1] - kernel.shape[1] + 1]\n","    output = np.zeros(output_shape)\n","\n","    #iterating over the input image\n","    for i in range(output.shape[0]):\n","        for j in range(output.shape[1]):\n","            #extract the 3 x 3 patch of the image at position i,j and aply kernel\n","            #as shown in fig 3\n","            output[i, j] = np.sum(input[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel)\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Slq77Auz2v95"},"outputs":[],"source":["# this function is used for back pass\n","def image_convolution(input, kernel, mode='full'):\n","    #this flips the kernel left/right followed by up/down\n","    kernel = np.flipud(np.fliplr(kernel))\n","\n","    # padding ensures the output size is input_size + kernel_size - 1\n","    # retains the original size\n","    if mode == 'full':\n","    # Adds kernel_height - 1 pixels top/bottom.\n","    # Adds kernel_width - 1 pixels left/right.\n","        padded_input = np.pad(input, [(kernel.shape[0]-1, kernel.shape[0]-1), (kernel.shape[1]-1, kernel.shape[1]-1)], mode='constant')\n","    else:\n","        padded_input = input\n","    output_shape = [padded_input.shape[0] - kernel.shape[0] + 1, padded_input.shape[1] - kernel.shape[1] + 1]\n","    # initializing the output array.\n","    output = np.zeros(output_shape)\n","\n","    for i in range(output.shape[0]):\n","        for j in range(output.shape[1]):\n","            # output os the  the product of overlapping elements between the padded input and kernel.\n","            output[i, j] = np.sum(padded_input[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel)\n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pVRny9-f2xfw"},"outputs":[],"source":["def image_convolution(input, kernel, mode='full'):\n","    #this flips the kernel for kernel operations\n","    kernel = np.flipud(np.fliplr(kernel))\n","    #padding the input image around the edges.\n","    if mode == 'full':\n","        padded_input = np.pad(input, [(kernel.shape[0]-1, kernel.shape[0]-1), (kernel.shape[1]-1, kernel.shape[1]-1)], mode='constant')\n","    else:\n","        padded_input = input\n","\n","    output_shape = [padded_input.shape[0] - kernel.shape[0] + 1, padded_input.shape[1] - kernel.shape[1] + 1]\n","    # initializing the output array.\n","    output = np.zeros(output_shape)\n","\n","    for i in range(output.shape[0]):\n","        for j in range(output.shape[1]):\n","            # output os the  the product of overlapping elements between the padded input and kernel.\n","            output[i, j] = np.sum(padded_input[i:i+kernel.shape[0], j:j+kernel.shape[1]] * kernel)\n","\n","    return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29bjuj8f2y04"},"outputs":[],"source":["class convolution:\n","    def __init__(self, input_shape, kernel_size, depth):\n","        input_depth, input_height, input_width = input_shape\n","        #defining the attributes for forward and backward methods\n","        self.depth = depth\n","        self.input_shape = input_shape\n","        self.input_depth = input_depth\n","        self.output_shape = (depth, input_height - kernel_size + 1, input_width - kernel_size + 1)\n","        self.kernels_shape = (depth, input_depth, kernel_size, kernel_size)\n","\n","        #initialize randomly initially\n","        self.kernels = np.random.randn(*self.kernels_shape)\n","        self.biases = np.random.randn(*self.output_shape)\n","\n","\n","    def forward(self, input):\n","        self.input = input\n","        self.output = np.copy(self.biases)\n","        for i in range(self.depth):\n","            for j in range(self.input_depth):\n","                #input image correlation with correspoding kernels\n","                self.output[i] += image_correlation(self.input[j], self.kernels[i, j], 'valid')\n","        return self.output\n","\n","    def backward(self, output_gradient, learning_rate):\n","        kernels_gradient = np.zeros(self.kernels_shape)\n","        input_gradient = np.zeros(self.input_shape)\n","\n","        for i in range(self.depth):\n","            for j in range(self.input_depth):\n","                kernels_gradient[i, j] = image_correlation(self.input[j], output_gradient[i], \"valid\")\n","                #computing the gradients with respect to ip\n","                input_gradient[j] += image_convolution(output_gradient[i], self.kernels[i, j], \"full\")\n","\n","        #updates\n","        self.kernels -= learning_rate * kernels_gradient\n","        self.biases -= learning_rate * output_gradient\n","        return input_gradient\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0m2ib_TF23H2"},"outputs":[],"source":["class sigmoid:\n","    def forward(self, x):\n","        self.output = 1 / (1 + np.exp(-x))\n","        return self.output\n","\n","    def backward(self, output_gradient):\n","        # The derivative of the sigmoid function is sigmoid(x) * (1 - sigmoid(x))\n","        return output_gradient * self.output * (1 - self.output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cpAsDIB425yE"},"outputs":[],"source":["class average_pooling:\n","    # Initializing the average pooling layer\n","    def __init__(self, input_shape=(2, 26, 26), pool_size=2, stride=2):\n","        # Storing the  parameters\n","        self.input_shape = input_shape\n","        self.pool_size = pool_size\n","        self.stride = stride\n","\n","        # Calculating the output shape based on the input shape, pool size, and stride\n","        self.output_shape = (\n","            input_shape[0],\n","            (input_shape[1] - pool_size) // stride + 1,\n","            (input_shape[2] - pool_size) // stride + 1\n","        )\n","\n","    # Implementing the forward pass for the pooling operation\n","    def forward(self, input):\n","        # Storing the input for potential future use\n","        self.input = input\n","\n","        # Extracting the dimensions of the input\n","        depth, input_height, input_width = self.input_shape\n","\n","        # Initializing an array for the output with the calculated output shape\n","        output = np.zeros(self.output_shape)\n","\n","        # Iterating through each depth slice of the input\n","        for d in range(depth):\n","            # Iterating over the input's height and width with the specified stride\n","            for i in range(0, input_height, self.stride):\n","                for j in range(0, input_width, self.stride):\n","                    # Defining the current window fur pooling\n","                    h_start, h_end = i, i + self.pool_size\n","                    w_start, w_end = j, j + self.pool_size\n","\n","                    # Extracting the current window from the ip\n","                    window = input[d, h_start:h_end, w_start:w_end]\n","\n","                    # Calculating the average value of the current window\n","                    # and storing it in the corresponding position in the output\n","                    output[d, i // self.stride, j // self.stride] = np.mean(window)\n","\n","        # Returning the op after pooling\n","        return output\n","\n","\n","    def backward(self, output_gradient):\n","        depth, input_height, input_width = self.input_shape\n","        input_gradient = np.zeros(self.input_shape)\n","\n","        for d in range(depth):\n","            for i in range(0, input_height, self.stride):\n","                for j in range(0, input_width, self.stride):\n","                    h_start, h_end = i, i + self.pool_size\n","                    w_start, w_end = j, j + self.pool_size\n","\n","                    # Distribute the gradient equally to each element in the window\n","                    input_gradient[d, h_start:h_end, w_start:w_end] += \\\n","                        output_gradient[d, i // self.stride, j // self.stride] / (self.pool_size * self.pool_size)\n","\n","        return input_gradient\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLU8fK1q29mN"},"outputs":[],"source":["class flatten_layer:\n","    def forward(self, input):\n","        self.input_shape = input.shape\n","        return input.flatten()\n","\n","    def backward(self, output_gradient):\n","        return output_gradient.reshape(self.input_shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEQdzuY52-Im"},"outputs":[],"source":["class dense_layer:\n","    def __init__(self, input_size, output_size):\n","        self.weights = np.random.randn(output_size, input_size)\n","        self.biases = np.random.randn(output_size)\n","\n","    def forward(self, input):\n","        self.input = input\n","      #  print(self.input.shape)\n","        eval = np.dot(self.weights, input) + self.biases\n","      #  print(eval.shape) = 10,\n","        return eval\n","\n","    def backward(self, output_gradient, learning_rate):\n","        # Gradient of loss w.r.t. weights\n","        weights_gradient = np.dot(output_gradient.reshape(-1, 1), self.input.reshape(1, -1))\n","\n","        # Gradient of loss w.r.t. biases\n","        biases_gradient = output_gradient\n","\n","        # Gradient computation of loss w.r.t. input\n","        input_gradient = np.dot(self.weights.T, output_gradient)\n","\n","        # Updating weights and biases\n","        self.weights -= learning_rate * weights_gradient\n","        self.biases -= learning_rate * biases_gradient\n","\n","        return input_gradient"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4Npt0PW3AEX"},"outputs":[],"source":["class softmax:\n","    def forward(self, input):\n","        exps = np.exp(input - np.max(input, axis=-1, keepdims=True))\n","        self.output = exps / np.sum(exps, axis=-1, keepdims=True)\n","        return self.output\n","\n","# Softmax backward is a pass-through because:\n","    def backward(self, dL_dZ):\n","        return dL_dZ\n","        #just propagate p-t as downstream!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RF-ayyMD3BZd"},"outputs":[],"source":["#one hot code for all the labels\n","def one_hot_encode(y):\n","    one_hot = np.zeros((y.size, y.max()+1))\n","    one_hot[np.arange(y.size), y] = 1\n","    return one_hot\n","\n","#Eg: y = np.array([0, 2, 1, 2]) becomes:\n","#[[1 0 0]\n","#[0 0 1]\n","#[0 1 0]\n","#[0 0 1]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzgqubda3CZ1"},"outputs":[],"source":["class cross_entropy_loss:\n","    def __init__(self):\n","        pass\n","\n","    def compute_loss(self, t_list, p_list):\n","        # Ensure inputs are 2D arrays\n","        t_list = np.atleast_2d(np.float_(t_list))\n","        p_list = np.atleast_2d(np.float_(p_list))\n","\n","        # Compute cross entropy loss\n","        # we dont want p_list to be 0 so adding some error term\n","        losses = -np.sum(t_list * np.log(p_list + 1e-15), axis=1)\n","        return np.mean(losses)\n","\n","    def compute_dloss(self, t_list, p_list):\n","        return p_list - t_list"]},{"cell_type":"markdown","source":["### Load data and create labels"],"metadata":{"id":"H7gYerWdbCJo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6DlkVg23DSD"},"outputs":[],"source":["# Load MNIST dataset and apply one hot encoding to the labels\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","#batch, num_imgs,w,h\n","train_images = train_images.reshape((-1, 1, 28, 28)) / 255.0\n","test_images = test_images.reshape((-1, 1, 28, 28)) / 255.0\n","\n","#one hot encode train and test labels\n","train_labels = one_hot_encode(train_labels)\n","test_labels = one_hot_encode(test_labels)\n"]},{"cell_type":"markdown","source":["### Instantiate Layers"],"metadata":{"id":"_7rLDquRbKGV"}},{"cell_type":"code","source":["# Initialize layers\n","cnn_layer = convolution(input_shape=(1, 28, 28), kernel_size=3, depth=2)\n","sigmoid_layer = sigmoid()  # Sigmoid layer after convolution\n","pooling_layer = average_pooling(input_shape=(2, 26, 26), pool_size=2, stride=2)\n","flatten_layer = flatten_layer()\n","dense_layer = dense_layer(input_size=13*13*2, output_size=10)\n","softmax_layer = softmax()\n","cross_entropy_loss = cross_entropy_loss()\n","\n","# Training parameters\n","num_epochs = 10\n","learning_rate = 0.01\n","\n","# Store results over time\n","train_accuracies = []\n","train_losses = []"],"metadata":{"id":"m5lvm2KDbNOQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train"],"metadata":{"id":"F40znTNAbRaL"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQwV5h4y3E5W","executionInfo":{"status":"ok","timestamp":1743420901287,"user_tz":-240,"elapsed":15003912,"user":{"displayName":"Niraj Pudasaini","userId":"08773241725886419596"}},"outputId":"802227e8-6ab0-4511-9246-ec3dee6a567f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Training Accuracy: 78.49%, Average Loss: 0.6870\n","Epoch 2, Training Accuracy: 88.71%, Average Loss: 0.3799\n","Epoch 3, Training Accuracy: 90.31%, Average Loss: 0.3289\n","Epoch 4, Training Accuracy: 91.53%, Average Loss: 0.2917\n","Epoch 5, Training Accuracy: 92.45%, Average Loss: 0.2603\n","Epoch 6, Training Accuracy: 93.21%, Average Loss: 0.2341\n","Epoch 7, Training Accuracy: 93.78%, Average Loss: 0.2128\n","Epoch 8, Training Accuracy: 94.24%, Average Loss: 0.1956\n","Epoch 9, Training Accuracy: 94.66%, Average Loss: 0.1818\n","Epoch 10, Training Accuracy: 95.03%, Average Loss: 0.1706\n"]}],"source":["# loop for epochs\n","for epoch in range(num_epochs):\n","    correct_train_predictions = 0\n","    total_loss = 0\n","\n","    #loop over all images\n","    for i in range(len(train_images)):\n","        image = train_images[i]\n","        label = train_labels[i]\n","\n","        # Forward pass\n","        cnn_output = cnn_layer.forward(image)\n","        sigmoid_output = sigmoid_layer.forward(cnn_output)\n","        pooling_output = pooling_layer.forward(sigmoid_output)\n","        flattened_output = flatten_layer.forward(pooling_output)\n","        dense_output = dense_layer.forward(flattened_output)\n","        predictions = softmax_layer.forward(dense_output)\n","\n","        #compute loss\n","        loss = cross_entropy_loss.compute_loss(label, predictions)\n","        total_loss += loss\n","\n","        # Check the prediction accuracy\n","        if np.argmax(predictions) == np.argmax(label):\n","            correct_train_predictions += 1\n","\n","        # Backward pass\n","        grad_back = cross_entropy_loss.compute_dloss(label, predictions)\n","        #print(grad_back)\n","        grad_back = softmax_layer.backward(grad_back)\n","        #print(grad_back)\n","        grad_back = dense_layer.backward(grad_back, learning_rate)\n","        #print(grad_back)\n","        grad_back = flatten_layer.backward(grad_back)\n","        #print(grad_back)\n","        grad_back = pooling_layer.backward(grad_back)\n","        #print(grad_back)\n","        grad_back = sigmoid_layer.backward(grad_back)\n","        #print(grad_back)\n","        grad_back = cnn_layer.backward(grad_back, learning_rate)\n","\n","    # Calculate average accuracy and loss for the epoch\n","    train_accuracy = correct_train_predictions / len(train_images)\n","    average_loss = total_loss / len(train_images)\n","    #print(average_losss)\n","    train_accuracies.append(train_accuracy)\n","    train_losses.append(average_loss)\n","\n","\n","    # Print summary after each epoch\n","    print(f\"Epoch {epoch + 1}, Training Accuracy: {train_accuracy * 100:.2f}%, Average Loss: {average_loss:.4f}\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kodvY2kF3Gk_"},"outputs":[],"source":["def test_model(cnn_layer, sigmoid_layer, pooling_layer, flatten_layer, dense_layer, softmax_layer, test_images, test_labels):\n","    correct_test_predictions = 0\n","    total_loss_test = 0\n","    for i in range(len(test_images)):\n","        # Forward pass with test data\n","        test_image = test_images[i]\n","        test_label = test_labels[i]\n","\n","        test_cnn_output = cnn_layer.forward(test_image)\n","        test_sigmoid_output = sigmoid_layer.forward(test_cnn_output)\n","        test_pooling_output = pooling_layer.forward(test_sigmoid_output)\n","        test_flattened_output = flatten_layer.forward(test_pooling_output)\n","        test_dense_output = dense_layer.forward(test_flattened_output)\n","        test_predictions = softmax_layer.forward(test_dense_output)\n","\n","        loss_test = cross_entropy_loss.compute_loss(test_label, test_predictions)\n","        total_loss_test += loss_test\n","\n","        # Check the prediction accuracy\n","        if np.argmax(test_predictions) == np.argmax(test_label):\n","            correct_test_predictions += 1\n","\n","    test_accuracy = correct_test_predictions / len(test_images)\n","    loss_final = total_loss_test / len(test_images)\n","    return test_accuracy,loss_final\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7DpHIypI3IfH","executionInfo":{"status":"ok","timestamp":1743421024861,"user_tz":-240,"elapsed":123575,"user":{"displayName":"Niraj Pudasaini","userId":"08773241725886419596"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9aa77802-5a81-4dca-8ffc-588a0c0e761a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Testing Accuracy: 95.24%, Testting Loss: 0.1700\n"]}],"source":["test_accuracy,loss_final = test_model(cnn_layer, sigmoid_layer, pooling_layer, flatten_layer, dense_layer, softmax_layer, test_images, test_labels)\n","print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%, Testting Loss: {loss_final:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOjhGvPV3JgM"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDEYiu71zs2UqgV48yG0y5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}